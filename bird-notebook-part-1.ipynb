{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ****Step 1: Import Modules****\nTo begin, we need to import all of the necessary modules for our project. Run the code cell below to import the various pytorch modules, pretrained models, etc. that we will need.\n\nIn addition, we will also attempt to use a GPU device if we can get access, as that will significantly speed up the runtime of training our models.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport PIL\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Models to import\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom torchvision.models import resnet152, ResNet152_Weights\nfrom torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\nfrom torchvision.models import densenet201, DenseNet201_Weights\n\ndevice = torch.device(1)\nprint(torch.cuda.get_device_name(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Step 2: Get Data****\nNext step is to get the data needed for training and testing. We define a method called `getData` below, which will create DataLoaders for the testing and training dataset and return them. We need to split our training directory into a 80/20 split, where 80% of the data will be used for training, and 20% of the data will be used for testing. \n\nBy default, we will use a batch size of 64 and an image size of 3 x 224 x 224 for training set, although this may be changed as we fine-tune our models.","metadata":{}},{"cell_type":"code","source":"def getData(batch = 64, shuffle = True, transform_train = None, transform_test = None, size = 224):\n    if (transform_train is None):\n        transform_train = transforms.Compose([\n            transforms.Resize((size, size)),\n            transforms.ToTensor(),\n        ])\n    \n    if (transform_test is None):   \n        transform_test = transforms.Compose([\n            transforms.Resize((size, size)),\n            transforms.ToTensor(),\n        ])\n\n    trainset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23wi/birds/train', transform = transform_train)\n    testset = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23wi/birds/train', transform = transform_test)\n\n\n    # Split the trainset into testing and training sets, we will use a manual seed for consistency\n    trainingSet , _ = torch.utils.data.random_split(trainset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n    _ , testingSet = torch.utils.data.random_split(testset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n    \n    \n    trainLoader = torch.utils.data.DataLoader(trainset, batch_size=batch, shuffle=True, num_workers=2)\n    testLoader = torch.utils.data.DataLoader(testingSet, batch_size=1, shuffle=False, num_workers=2)\n    \n    finalSet = torchvision.datasets.ImageFolder(root='/kaggle/input/birds23wi/birds/test', transform=transform_test)\n    finalLoader = torch.utils.data.DataLoader(finalSet, batch_size=1, shuffle=False, num_workers=2)\n    \n    classes = open(\"/kaggle/input/birds23wi/birds/names.txt\").read().strip().split(\"\\n\")\n    class_to_idx = trainset.class_to_idx\n    idx_to_class = {int(v): int(k) for k, v in class_to_idx.items()}\n    idx_to_name = {k: classes[v] for k,v in idx_to_class.items()}\n    \n    return {'train': trainLoader, 'test': testLoader, 'final': finalLoader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n\ndata = getData()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Step 3: Visualize Data****\nWe can visualize a subset of our images by calling the iterator on the training set's DataLoader. We can also see the size of the images, which should be 3 x 224 x 224 at this point. ","metadata":{}},{"cell_type":"code","source":"dataiter = iter(data['train'])\nimages, labels = next(dataiter)\nimages = images[:8]\nprint(images.size())\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# show images\nimshow(torchvision.utils.make_grid(images))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Step 4: Training and Prediction****\nThe following `train` function defines a training procedure for the neural network. The training function takes in an optimizer, which determines the decay, momentum, and learning rate of the model. Note that the default optimizer is stochastic gradient descent with parameters lr=0.01, momentum=0.9, decay=0.0005. It also takes in the network to train and the DataLoader with the prepared data. This function will also save checkpoints to the file path given by checkpoint_path, so that the model can be reloaded after training easily. ","metadata":{}},{"cell_type":"code","source":"# Function to train model\ndef train(net, dataloader, epochs=1, start_epoch=0, optimizer = None, \n          verbose=1, print_every=10, state=None, schedule={}, checkpoint_path=None):\n    net.to(device)\n    net.train()\n    losses = []\n    criterion = nn.CrossEntropyLoss()\n    # We will use SGD for the default optimizer\n    if (optimizer is None):    \n        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n\n    # Load previous training state\n    if state:\n        net.load_state_dict(state['net'])\n        optimizer.load_state_dict(state['optimizer'])\n        start_epoch = state['epoch']\n        losses = state['losses']\n\n    # Fast forward lr schedule through already trained epochs\n    for epoch in range(start_epoch):\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n    for epoch in range(start_epoch, epochs):\n        sum_loss = 0.0\n\n        # Update learning rate when scheduled\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n        for i, batch in enumerate(dataloader, 0):\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()  # autograd magic, computes all the partial derivatives\n            optimizer.step() # takes a step in gradient direction\n\n            losses.append(loss.item())\n            sum_loss += loss.item()\n\n            if i % print_every == print_every-1:    # print every 10 mini-batches\n                if verbose:\n                    print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n                sum_loss = 0.0\n        if checkpoint_path:\n            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n            torch.save(state, checkpoint_path + 'checkpoint-%d.pkl'%(epoch+1))\n    return losses","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we define some auxilary functions to help analyze our models. \n\nThe ```predict``` function will be used to create a csv file\nwith the predictions of the given network on the give dataset. This will be used to create the final ```submissions.csv``` file that will\nhave the predictions for the testing set.\n\nThe ```accuracy``` function will return the accuracy of the given network on the given dataloader. It will return a float\nof the proportion of correct predictions.\n\nThe ```smooth``` function will simply smooth out the given list of losses using convolution. It is a utility function to help\nvisualize graphs more smoothly.","metadata":{}},{"cell_type":"code","source":"# Function to predict data in given DataLoader. Will output results\n# to a csv file titled with the give ofname\ndef predict(net, dataloader, ofname):\n    out = open(ofname, 'w')\n    out.write(\"path,class\\n\")\n    net.to(device)\n    net.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            if i%100 == 0:\n                print(i)\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            fname, _ = dataloader.dataset.samples[i]\n            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n    out.close()\n\n# Function to get the accuracy of the given network on the given dataloader.\n# Returns a single float representing the proportion of correct\n# predictions.\ndef accuracy(net, dataloader):\n    net.to(device)\n    net.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(dataloader, 0):\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    net.train()\n    return correct/total\n\n# Function to smooth losses before graphing\ndef smooth(x, size):\n    return np.convolve(x, np.ones(size)/size, mode='valid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Step 5: Determine Architecture****\n\n<font size=\"4\"><b>Baseline: ResNet18</b></font>\n\nFinally, after all the setup, we can begin training our models. The first step we will take is to determine the best architecture for our model. First, let's have a baseline model that we can compare our other models against.\n\nWe can use ResNet18 as our baseline, which is a convolutional network with residual connections. We will use the weights that have already been pretrained on the ImageNet dataset, so that we just need to finetune on our birds dataset.\n\nTo save GPU compute, we will train for just 3 epochs. \n\nThe following code will train the model and output the losses, as well as graph the losses and output the predictions on the testing dataset.","metadata":{}},{"cell_type":"code","source":"# Save checkpoints to directory\ncheckpoint_path = '/kaggle/working/resnet18_1/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n\n# Train ResNet18 model\nresnet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\nresnet18.fc = nn.Linear(resnet18.fc.in_features, 555)\nlosses = train(resnet18, data['train'], epochs=3, print_every=10, checkpoint_path=checkpoint_path)\n\n# Print loss graph and make predictions\nplt.plot(smooth(losses,50))\n\nprint(accuracy(resnet18, data['test']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On our first run through, we got a loss of 1.208 after 3 epochs!\n\nIn addition, our testing accuracy is 0.50900. That's a pretty good score, but we could probably do better with a different architecture. For this project, we will test three different neural network architectures pretrained on ImageNet: DenseNet, ResNet152, and EfficientNet.\n\n<font size=\"4\"><b>Testing: ResNet152</b></font>\n\nNext, we will try the ResNet152 architecture, which is a residual CNN with 152 layers! ","metadata":{}},{"cell_type":"code","source":"# Save checkpoints to directory\ncheckpoint_path = '/kaggle/working/resnet152_1/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n\n# Train ResNet152 model\nresnet152 = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\nresnet152.fc = nn.Linear(resnet152.fc.in_features, 555)\nlosses = train(resnet152, data['train'], epochs=3, print_every=10, checkpoint_path=checkpoint_path)\n\n# Print loss graph and make predictions\nplt.plot(smooth(losses,50))\n\n# Print the accuracy of the model on the testing set\naccuracy(resnet152, data['test'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We had a loss of 1.074 and a testing accuracy of 0.4542271784232365! Interestingly, our testing accuracy decreased compared to ResNet18, even though ResNet152 is far more powerful. This may be because ResNet152 has far more layers, so it requires more training time to converge to the optimum. Unfortunately, our GPU compute time is limited, so we will leave ResNe152 as it is.\n\n<font size=\"4\"><b>Testing: DenseNet</b></font>\n\nLet's try the DenseNet architecture this time.","metadata":{}},{"cell_type":"code","source":"# Save checkpoints to directory\ncheckpoint_path = '/kaggle/working/densenet_1/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n\n# Train DenseNet201 model\ndensenet = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\ndensenet.fc = nn.Linear(densenet.fc.in_features, 555)\nlosses = train(densenet, data['train'], epochs=3, print_every=10, checkpoint_path=checkpoint_path)\n\n# Print loss graph and make predictions\nplt.plot(smooth(losses,50))\n\naccuracy(densenet, data['test'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On our runthrough, DenseNet had a loss of 1.032 after 3 epochs and a testing accuracy of 0.5272302904564315! That's a huge improvement over both ResNet18 and ResNet152. However, we should also try EfficientNet next, since this particular model is both small and has a very high Top 1 accuracy on ImageNet.\n\n<font size=\"4\"><b>Testing: EfficientNet</b></font>\n\nFinally, let's try the EfficientNet architecture.","metadata":{}},{"cell_type":"code","source":"# Save checkpoints to directory\ncheckpoint_path = '/kaggle/working/efficentnet_1/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n\n# Train EfficentNet2 model\neffnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\neffnet.classifier[1] = nn.Linear(effnet.classifier[1].in_features, 555)\nlosses = train(effnet, data['train'], epochs=3, print_every=10, checkpoint_path=checkpoint_path)\n\n# Print loss graph and make predictions\nplt.plot(smooth(losses,50))\n\naccuracy(effnet, data['test'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! We had a loss of 0.838 and a testing accuracy of 0.649896265560166! That's a massive improvement over DenseNet, and clearly the best model out of all the models we tested. We will choose the EfficientNet model for the next portion of experiments, and it will be the final model that we use to train the classifier.","metadata":{}},{"cell_type":"markdown","source":"# ****Step 6: Data Augmentation****\n\n<font size=\"4\"><b>Introduction</b></font>\n\nNext up! Data augmentation. Our dataset is fairly small, so we can artificially increase the data size by using data augmentation. There are many different techniques possible, and in this step we will consider a few popular techniques that have good success on image classification tasks. \n\nTo help visualize the augmentation, we will define some utility functions, ```VisualizeTransform``` and ```DefaultTransform```. ```VisualizeTransform``` will be used to visualize the transformed image. The function will take in a transform and graph a random transformed image from the given dataloader. The image will be sampled from the given rootDir, which must be a directory with at least 1 training image. \n\nNext, ```DefaultTransform``` will return a default transform that simply resizes the image to 224 x 224. This is simply to help visualize the transformations and compare against the default.","metadata":{}},{"cell_type":"code","source":"# Default Transform\ndef DefaultTransform(size = 224):\n    defaultTransform = transforms.Compose([\n        transforms.Resize((size,size)),\n        transforms.ToTensor()\n    ])\n    return defaultTransform\n\n# Visualize Transform\ndef VisualizeTransform(transform = DefaultTransform(), rootDir = '/kaggle/working/visualize_img'):\n    beforeImage = torchvision.datasets.ImageFolder(root=rootDir, transform=DefaultTransform())\n    beforeLoader = torch.utils.data.DataLoader(beforeImage, batch_size=1, shuffle=True, num_workers=2)\n\n    afterImage = torchvision.datasets.ImageFolder(root=rootDir, transform=transform)\n    afterLoader = torch.utils.data.DataLoader(afterImage, batch_size=1, shuffle=True, num_workers=2)\n\n    beforeiter = iter(beforeLoader)\n    before, beforeLabel = next(beforeiter)\n    \n    afteriter = iter(afterLoader)\n    after, afterLabel = next(afteriter)\n    \n    def imshow(img):\n        npimg = img.numpy()\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        plt.show()\n        \n    imshow(torchvision.utils.make_grid(before))\n    imshow(torchvision.utils.make_grid(after))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Random Flip and Rotation</b></font>\n\nFirst, we will consider a simple technique, the linear transformation. In particular, we will consider the horiziontal flip, the vertical flip, and the rotation as our primary transformations. \n\nThe structure of the dataset matters here. Specifically, we may not want a vertical flip, since birds are generally not viewed upside down. However, a horizontal flip and some rotation may be appropriate to make our model more robust against noise.\n\nTo implement flipping and rotation, we define the following function, ```RandomFlip```. This function will take in four parameters, horP, verP, rotDeg, and rotP. The horP and verP will determine the probability of horizontal and vertical flips, respectively. Then, rotP will determine the probability of rotation, and rotDeg will determine by how much the image is rotated. ","metadata":{}},{"cell_type":"code","source":"# Function for returning a Transform with three possible transforms:\n# - vertical flip\n# - horizontal flip\n# - rotational flip\ndef RandomFlip(horP = 1, verP = 0, rotDeg = 3, rotP = 0.5):        \n    retTransform = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=horP),\n        transforms.RandomVerticalFlip(p=verP),\n    ])\n    \n    rotationProb = np.random.ranf()\n    if (rotationProb < rotP):\n        retTransform = transforms.Compose([\n            retTransform,\n            transforms.RandomRotation(degrees = rotDeg)      \n        ])\n        \n    return retTransform\n\nflip = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomFlip(),\n    transforms.ToTensor()\n])\n\nVisualizeTransform(transform = flip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Random Occlusion</b></font>\n\nAccording to this paper: https://arxiv.org/pdf/1708.04896.pdf, another technique that works well with image classification is Random Occlusion. This is a technique that randomly chooses a box in the image to cover with noisy pixels. The idea is that it will make the network more robust to occlusion in the testing dataset, as it will have been trained on images that already have various components of the object covered.\n\nTo implement random occlusion, we define the following function, ```RandomOcclusion```. This function will take in 5 parameters: p, r1, r2, sl, sh. The p determines the probability for each image that occlusion is applied at all. Then, r1 and r2 is the range in which a random aspect ratio R will be chosen, where R is the ratio between the height and width of occlusion. Finally, sl and sh is the range in which a random value S will be chosen, where S is the proportion of the total area of the image that is covered.\n","metadata":{}},{"cell_type":"code","source":"def RandomOcclusion(p = 0.5, r1 =0.3, r2 = 3.33, sl = 0.02, sh = 0.3):\n    def image_occlude(image):\n        # With probability p, do not transform image\n        if np.random.ranf() < p:\n            return image\n        \n        # Choose random Re and Se         \n        w = image.width\n        h = image.height\n                \n        randS = np.random.ranf() * (sh - sl) + sl\n        Se = h * w * randS\n        Re = np.random.ranf() * (r2 - r1) + r1\n        \n        He = int(np.sqrt(Se * Re))\n        We = int(np.sqrt(Se / Re))\n        \n        # Verify that the area is no more than 40%\n        # of total image, otherwise pass\n        if (We * He > w * h * 0.4):\n            return image\n        \n        # Select random pixel and write box\n        x = np.random.randint(0, max(w - We, 2))\n        y = np.random.randint(0, max(h - He, 2))\n        if (x + We <= w and y + He <= h):\n            for i in range(x, x + We):\n                for j in range(y, y + He):\n                    image.putpixel((i,j),(np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256)))\n                    \n        return image\n    \n    totalTransform = transforms.Compose([\n        transforms.Lambda(image_occlude),\n    ])\n    \n    return totalTransform\n\nocclusion = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomOcclusion(),\n    transforms.ToTensor()\n])\nVisualizeTransform(transform = occlusion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Random Noise</b></font>\n\nAnother technique that might help our model be come more robust is adding some random noise. This means that we will change each pixel by some slight offset randomly. This might make our model overfit less on noisy pixels found in the training set.\n\nTo implement random noise, we define the following function, ```RandomNoise```. This function will take in 3 parameters: p, r1, r2. The p determines the probability for each pixel that noise will be added. Then, a value R will be chosen uniformly from the range r1 to r2. This R value will then be added to each R,G,B value of the pixel as the random offset. ","metadata":{}},{"cell_type":"code","source":"# Define function that will add some random \n# amount of noise to each pixel. Uses a uniform distribution\n# sample to generate noise\n# r1, r2 is range for uniform distribution\n# p is probability that a pixel will have noise added\ndef RandomNoise(r1 = -30, r2 = 30, p = 0.3):\n    \n    def image_noise(image):\n        w = image.width\n        h = image.height\n        # For each pixel, need to add some random offset\n        # Generate some random noise\n        for i in range(h):\n            for j in range(w):\n                # Skip pixel with p probability\n                if (np.random.ranf() < p):\n                    continue\n                    \n                # Select random offsets\n                r = np.random.randint(r1, r2 + 1)\n                g = np.random.randint(r1, r2 + 1)\n                b = np.random.randint(r1, r2 + 1)\n                                \n                oldPix = image.getpixel((i,j))\n                newPix = tuple(np.add(oldPix, (r,g,b)))\n                image.putpixel((i,j), newPix)\n                \n        return image\n    \n    totalTransform = transforms.Compose([\n        transforms.Lambda(image_noise),\n    ])\n    \n    return totalTransform \n\nnoise = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomNoise(),\n    transforms.ToTensor()\n])\nVisualizeTransform(transform = noise)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Random Scaled Crop</b></font>\n\nOne thing that we can see in our dataset is that many of the birds are centered directly in the middle of the image. There are a lot of background pixels that don't have anything to do with the bird around the edges of the image.\n\nWe could take advantage of this fact by trying to crop out a portion of the edges of the image to make the central portion larger. This will increase the fidelity of the bird portion of the image, which might give our network more information to work with in classifying images. \n\nTo implement random noise, we define the following function, ```RandomScaledCrop```. This function will take in 3 parameters: p, marginL, marginH. The p determines the probability that the image will recieve the scaled crop transformation at all. Then, a random value M between marginL and marginH will be chosen uniformly. This M value will then be the percent of the edges that are cutoff. As an example, a M value of 0.1 means that 10% of the width and height will be removed from the edges.","metadata":{}},{"cell_type":"code","source":"# Function for returning a Random Centered Crop of the image. The function gives the option to specify\n# the size of the crop by percent area, as well as specifying a percent margin range to center the crop in.\ndef RandomScaledCrop(p = 1, marginL = .20, marginH = 0.25):\n    \n    def image_crop(image):\n        w = image.width\n        h = image.height\n        \n        # Calculate a random offset between margin range\n        # which will act as padding against all four sides\n        randRange = np.random.ranf() * (marginH - marginL) + marginL\n\n        widthPadd = w * randRange\n        heightPadd = h * randRange\n        \n        # Define two corners of the crop\n        lowerH = heightPadd\n        lowerW = widthPadd\n        upperH = h - heightPadd\n        upperW = w - widthPadd\n        \n        image = image.crop((lowerH, lowerW, upperH, upperW))        \n    \n        return image\n    \n    # First verify that inputs are reasonable, otherwise\n    # just return the default transform\n    if marginH >= 0.5 or marginL > marginH:\n        return None\n    if (np.random.ranf() > p):\n        return None\n    \n    totalTransform = transforms.Compose([\n        transforms.Lambda(image_crop),\n    ])\n    \n    return totalTransform  \n\nscaledCenterCrop = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomScaledCrop(),\n    transforms.ToTensor()\n])\nVisualizeTransform(transform = scaledCenterCrop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Random Stitch</b></font>\n\nAnother thing that we can see in our dataset is that the bird area is very small compared to the background area. Ideally, we would prefer 50% or greater area with bird pixels, so the neural networks have a lot of relevant information to train on. We can try to counteract this by multiplying the bird area relative to the background area. This can be done by taking a crop of the bird and stitching multiple crops of the bird together. \n\nTo implement random stitch, we define the following function, ```RandomStitch```. This function will take in 2 parameters: p, side. The p determines the probability that the image will a stitch transformation at all. Then, side will be the number of boxes in the final stitch. The random stitch will then be constructed from several random crops stitched together. ","metadata":{}},{"cell_type":"code","source":"# Function for returning a Random Stitch of the image. The function gives the option to specify\n# probability that stitch will be applied, as well as specifying the number of sides to stitch\ndef RandomStitch(p = 1, side = 2):\n    def image_stitch(image):\n        w = image.width\n        h = image.height\n        \n        # We want side number of stitches\n        wPerSide = w // side\n        hPerSide = h // side\n        \n        # First choose a random offset to start from\n        randW = np.random.randint(0, min(w - wPerSide, w)) \n        randH = np.random.randint(0, min(h, h - hPerSide))\n        \n        newImage = PIL.Image.new(mode = 'RGB', size = (h, w))\n        \n        # For each side, and for each box, write out the \n        for i in range(side):\n            for j in range(side):\n                randW = np.random.randint(0, w - wPerSide)\n                randH = np.random.randint(0, h - hPerSide)\n                for x in range(wPerSide * i, wPerSide * (i+1)):\n                    for y in range(hPerSide * j, hPerSide * (j+1)):\n                        # Error checking to make sure that image is within bounds\n                        if (randW + x - wPerSide * i >= 224 or randH + y - hPerSide * j >= 224):\n                            print((randW,randH))\n                            print((randW + x - wPerSide * i, randH + y - hPerSide * j))\n                            continue\n                            \n                        pix = image.getpixel((randW + x - wPerSide * i, randH + y - hPerSide * j))\n                        newImage.putpixel((x,y), pix)\n                        \n                        \n        return newImage\n                                    \n    \n    # First verify that inputs are reasonable, otherwise\n    # just return the default transform\n    if side <= 0:\n        return None\n    if (np.random.ranf() > p):\n        return None\n    \n    totalTransform = transforms.Compose([\n        transforms.Lambda(image_stitch),\n    ])\n    \n    return totalTransform  \n\nrandomStitch = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomStitch(),\n    transforms.ToTensor()\n])\nVisualizeTransform(transform = randomStitch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Experiments</b></font>\n\nNow that we've got our basic augmentations written out, we need to test them on our data set. The following code sets up a test suite that tests each individual augmentation against a baseline model with no augmentation.\n\nFor each augmentation, we will apply it to a 224x224 image, then train it on EfficientNet2 for 6 epochs. Then, we will test the accuracy of the model on the testing set. We will also graph the losses of each model for visualization.\n\nAfterwards, we will re-evaluate the results and see which augmentations were effective, and which were not. Then, we will need to decide which ones we will keep.\n\nIn addition to our custom transformations, we also define some additional ones using the standard pytorch library. We will also experiment with changing the colorspace (brightness, saturation, hue), the sharpness, and normalizing the dataset against a common mean and standard deviation. \n\nAs a final note, unfortunately we did not have time to test all possible parameters, as our GPU compute and time is limited. We chose a select subset of parameters that we thought would be a good fit, and tested on that. However, it is entirely possible that a better set of parameters exist, which we did not test.","metadata":{}},{"cell_type":"code","source":"# Default\ncheckpoint_path = '/kaggle/working/default/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n\n# Occlusion\ncheckpoint_path = '/kaggle/working/occlusion/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomOcclusion(),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n\n# Flip\ncheckpoint_path = '/kaggle/working/flip/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomFlip(),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# ScaledCenterCrop\ncheckpoint_path = '/kaggle/working/SCC/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    RandomScaledCrop(),\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Sharpness\ncheckpoint_path = '/kaggle/working/sharpness/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomAdjustSharpness(2),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# ColorJitter\ncheckpoint_path = '/kaggle/working/jitter/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Test Normalize\ncheckpoint_path = '/kaggle/working/normalize/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransformTrain = transforms.Compose([\n    DefaultTransform(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransformTest = transforms.Compose([\n    DefaultTransform(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndata = getData(transform_train = transformTrain, transform_test = transformTest)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Random Noise\ncheckpoint_path = '/kaggle/working/noise/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomNoise(),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Random Stitch\ncheckpoint_path = '/kaggle/working/stitch/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    RandomStitch(),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"><b>Results</b></font>\n\nAccording to our results, we had the following testing accuracy measurements:\n\n**Default:** 0.7868257261410788\n\n**Random Occlusion:** 0.7916234439834025\n\n**Random Flip:** 0.8001815352697096\n\n**Random Scaled Crop:** 0.7547977178423236\n\n**Sharpness:** 0.7841026970954357\n\n**Random Color Jitter:** 0.7593360995850622\n\n**Normalize:** 0.79201244813278\n\n**Random Noise:** 0.619242738589212\n\n**Random Stitch:** 0.7141856846473029\n\nAccording to our experiments, the only augmentations that shows a definite testing accuracy improvement over the default base model is the Random Occlusion, Random Flip, and Normalize transformation. As such, we will add these to the final model, and see how they benefit the final testing accuracy. Quite disappointingly, our invented augmentations such as Random Stitch and Random Noise did significantly worse than we expected, but perhaps we need to tune the parameters better or train for longer.","metadata":{}},{"cell_type":"markdown","source":"# ****Step 7: Tuning Hyperparameters****\nFinally, our model is coming into focus. We will use a EfficientNet model, with Random Occlusion, Random Flip, and Normalize data augmentations. However, there are a couple of last steps to be taken. We still need to finetune the hyperparameters for the model that will be used during training. In particular, we will look at image input size, weight decay, and learning rate as the particular parameters that we will try to optimize.\n\n<font size=\"4\"><b>Image Size</b></font>\n\nOne of the questions that we still need to answer is how large our input image size should be. Intuitively, having a larger image input size should make the model better, since there is more information, but at the same time too large of an image input size may lead to underfitting, since now the image dimensions are too large for the model to learn effectively. Thus a balance must be achieved.\n\nFor our experiments, we will test four different image sizes: 224 x 224, 256 x 256, 384 x 384, and 516 x 516. Why 384 x 384? According to the original EfficientNet2 paper, the model was trained using a 384 x 384 central crop on the images. Thus, we thought it would be important to test that particular size. \n\nThe following code defines the experiment code. Each image size is input into an EfficientNet Model and then trained for 6 epochs. Then, the testing accuracy is calculated and output to standard out.","metadata":{}},{"cell_type":"code","source":"# Set up image pixel size experiments\n\n# 224\ncheckpoint_path = '/kaggle/working/image224/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 224),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform, size = 224)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# 256\ncheckpoint_path = '/kaggle/working/image256/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 256),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform, size = 256)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n\n# 384\ncheckpoint_path = '/kaggle/working/image384/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 384),\n    transforms.ToTensor()\n])\n\ndata = getData(batch = 16, transform_train = transform, size = 384)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n\n# 512\ncheckpoint_path = '/kaggle/working/image512/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 512),\n    transforms.ToTensor()\n])\n\ndata = getData(batch = 8, transform_train = transform, size = 512)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to our run through, the relative testing accuracies are as follows:\n\n**Image Size 224:** 0.7900674273858921\n\n**Image Size 256:** 0.8031639004149378\n\n**Image Size 384:** 0.8408973029045643\n\n**Image Size 512:** 0.8220954356846473\n\nFrom our results, we can deduce that an image size of 384 x 384 is by far the best choice. It has nearly a 2% lead over the next best image size, which was 512 x 512. \n\n<font size=\"4\"><b>Weight Decay</b></font>\n\nNext, we need to figure out the optimal weight decay for our model. The question of what weight decay is essentially the question of how complex our model should be. Should the model be allowed to fit a very complex model for the data, or should it be contrained in some way? We expect that a lower weight decay will be better, since the classification problem is very complex - there are 555 classes to fit with a very large 384 x 384 input size. \n\nFor our experiments, we will test four different weight decays: 0.1, 0.01, 0.001, 0.0001. The following code defines the experiment code. Each weight decay is tuned for an EfficientNet Model for a 224 x 224 image and then trained for 6 epochs. Then, the testing accuracy is calculated and output to standard out.","metadata":{}},{"cell_type":"code","source":"# Test different weight decays\n# 0.1\ncheckpoint_path = '/kaggle/working/decay1/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 224),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform, size = 224)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.1)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path, optimizer = optimizer)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Test different weight decays\n# 0.01\ncheckpoint_path = '/kaggle/working/decay01/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 224),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform, size = 224)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path, optimizer = optimizer)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n# Test different weight decays\n# 0.001\ncheckpoint_path = '/kaggle/working/decay001/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 224),\n    transforms.ToTensor()\n])\n\n\ndata = getData(transform_train = transform, size = 224)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path, optimizer = optimizer)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))\n\n\n# Test different weight decays\n# 0.0001\ncheckpoint_path = '/kaggle/working/decay0001/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransform = transforms.Compose([\n    DefaultTransform(size = 224),\n    transforms.ToTensor()\n])\n\ndata = getData(transform_train = transform, size = 224)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\nlosses = train(net, data['train'], epochs=6, schedule={0:.01, 3:.001}, print_every=10, checkpoint_path=checkpoint_path, optimizer = optimizer)\nplt.plot(smooth(losses,50))\nprint(accuracy(net, data['test']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to our run through, the relative testing accuracies are as follows:\n\n**Decay 0.1:** 0.0016856846473029046\n\n**Decay 0.01:** 0.38731846473029047\n\n**Decay 0.001:** 0.785658713692946\n\n**Decay 0.0001:** 0.7857883817427386\n\nIt looks like our hypothesis was mostly correct. It seems that a lower weight decay is the best choice for the model. This again makes sense, given the inherent complexity of the problem. For our model, we will choose a weight decay of 0.0001. \n\n<font size=\"4\"><b>Learning Rate</b></font>\n\nThe learning rate of our model is perhaps the most important hyperparameter to use, but also the hardest to optimize. It is difficult to say what learning rate is best, since we should also iteratively relax the learning rate as the epochs increase. The question of what schedule is best for the model is difficult to find by brute force search, so instead we will use a more hueristic approach. We will say that it is better to train the model with a larger learning rate in the beginning, and then decrease the rate by an order of magnitude every couple of epochs.\n\nThe schedule that we define for this problem is as follows: 4 epochs of 0.01 learning rate, 4 epochs of 0.001 learning rate, and 15 epochs of 0.0001 learning rate. The idea is that the model should converge quickly to a local optima in the first four epochs, then for the next 15 epochs it will slowly reach the final local optima over many iterations of small steps.\n\nThe key here is that we will use a principle of early stopping. Early stopping is a training principle that continuously calculates the testing accuracy for each epoch. At any point, if the model's accuracy decreases from the previous epoch, then training is stopped. This method can help prevent losses from overfitting. We will rewrite the `train` function to implement the early stopping mechanism.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Function to train model\ndef TrainEarlyStopping(net, dataloader, epochs=1, start_epoch=0, optimizer = None, \n          verbose=1, print_every=10, state=None, schedule={}, checkpoint_path=None):\n    net.to(device)\n    net.train()\n    losses = []\n    criterion = nn.CrossEntropyLoss()\n    # We will use SGD for the default optimizer\n    if (optimizer is None):    \n        optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n    \n    # Define current highest testing accuracy\n    testingAcc = 0\n    \n    # Load previous training state\n    if state:\n        net.load_state_dict(state['net'])\n        optimizer.load_state_dict(state['optimizer'])\n        start_epoch = state['epoch']\n        losses = state['losses']\n\n    # Fast forward lr schedule through already trained epochs\n    for epoch in range(start_epoch):\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n    for epoch in range(start_epoch, epochs):\n        sum_loss = 0.0\n\n        # Update learning rate when scheduled\n        if epoch in schedule:\n            print (\"Learning rate: %f\"% schedule[epoch])\n            for g in optimizer.param_groups:\n                g['lr'] = schedule[epoch]\n\n        for i, batch in enumerate(dataloader, 0):\n            inputs, labels = batch[0].to(device), batch[1].to(device)\n\n            optimizer.zero_grad()\n\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()  # autograd magic, computes all the partial derivatives\n            optimizer.step() # takes a step in gradient direction\n\n            losses.append(loss.item())\n            sum_loss += loss.item()\n\n            if i % print_every == print_every-1:    # print every 10 mini-batches\n                if verbose:\n                    print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n                sum_loss = 0.0\n        if checkpoint_path:\n            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n            torch.save(state, checkpoint_path + 'checkpoint-%d.pkl'%(epoch+1))\n        \n        # If the testing accuracy decreases, then return early\n        currAcc = accuracy(net, testloader)\n        if (currAcc >= testingAcc):\n            testingAcc = currAcc\n        else:\n            print(\"Testing accuracy decreased at epoch \" + str(epoch))\n            return losses\n        \n    return losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****Step 8: Final Model and Results****\nWe've finally reached final model! The exact parameters are as follows:\n\n**Model:** EfficientNet2_s, pre-trained on ImageNet\n\n**Input Image Size:** 384 x 384\n\n**Learning Rate:** 4 epochs of 0.01 learning rate, 4 epochs of 0.001 learning rate, and 15 epochs of 0.0001 learning rate\n\n**Weight Decay:** 0.0001\n\n**Momentum:** 0.9\n\n**Batch Size:** 16\n\n**Augmentations:** Normalization, Random Occlusion, Random Flipping/Rotation\n\nThe code for implementing this model is shown below. \n\nWith this model, we had a final testing accuracy of 0.8559387966804979. Not bad, given the GPU compute and time constraints!","metadata":{}},{"cell_type":"code","source":"# Final\ncheckpoint_path = '/kaggle/working/final/'\nif not os.path.exists(checkpoint_path):\n    os.makedirs(checkpoint_path)\n    \nnet = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\nnet.classifier[1] = nn.Linear(net.classifier[1].in_features, 555)\n\ntransformTrain = transforms.Compose([\n    transforms.Resize((384,384)),\n    RandomFlip(),\n    RandomOcclusion(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntransformTest = transforms.Compose([\n    transforms.Resize((384,384)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ndata = getData(transform_train = transformTrain, transform_test = transformTest, size = 384, batch = 16)\nlosses = TrainEarlyStopping(net, data['train'], epochs=23, schedule={0:.01, 4:.001, 8:.0001}, print_every=10, checkpoint_path=checkpoint_path)\nplt.plot(smooth(losses,50))\n\npredict(net, data['final'], \"/kaggle/working/final/\" + \"submissions.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}